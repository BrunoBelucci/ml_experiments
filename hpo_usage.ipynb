{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3085c7",
   "metadata": {},
   "source": [
    "# ML Experiments HPO usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db5959",
   "metadata": {},
   "source": [
    "We will assume that the user is already familiar with the base usage, if not, please check the `base_usage.ipynb` notebook.\n",
    "\n",
    "The usage of ml_experiments for hyperparameter optimization (HPO) is very similar to the base usage, with the main difference being that we will use the `HPOExperiment` class instead of the `BaseExperiment` class. This class will automatically handle many parts of the HPO process by leveraging the `optuna` library. Similar to the base usage, we will need to define a class that inherits from `HPOExperiment`, the minimal implementation is as follows:\n",
    "\n",
    "```python\n",
    "class MyHPOExperiment(HPOExperiment):\n",
    "    def _get_combinations_names(self) -> list[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_extra_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _load_data(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_fn(self, trial_dict: dict, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_search_space(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_default_values(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> list:\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "Note that these are not the same methods needed for the `BaseExperiment`, that is because behind the scenes, the `HPOExperiment` inherits from the `BaseExperiment` class and already implements many of the needed methods for the base usage. The methods `_get_combinations_names`, `_get_extra_params` and `_load_data` are the same as in the base usage, so let's focus on the other methods.\n",
    "\n",
    "The `training_fn` method is the core of the HPO process, it is this function that will be called by the `optuna` library to train the model. It should return a dictionary which includes the metrics that we want to optmize. Let's illustrate by continuing with the previous example of training a `GradientBoostingClassifier` on the iris dataset. Imagine that we want to optimize the `learning_rate` parameter of the model by maximizing the `accuracy` metric using a bagging strategy where we divide several times the dataset in train and test set and we average the results. We will implement this logic in the `training_fn` method as follows:\n",
    "\n",
    "(Note that this is only a simplified example to showcase how to use the `HPOExperiment` class, in practice we probably do not want to use a bagging strategy like this, because of the 'data leakage' problem)\n",
    "\n",
    "```python\n",
    "def training_fn(self, trial_dict: dict, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "    n_bagging = unique_params[\"n_bagging\"]\n",
    "    trial = trial_dict[\"trial\"]\n",
    "    learning_rate = trial.params[\"learning_rate\"]\n",
    "    metrics = []\n",
    "    for i in range(n_bagging):\n",
    "        base_experiment = ClassificationExperiment(\n",
    "            seed=i,\n",
    "            n_estimators=100,\n",
    "            learning_rate=learning_rate,\n",
    "            model_verbose=0\n",
    "        )\n",
    "        result = base_experiment.run(return_results=True)\n",
    "        metrics.append(result[0][\"evaluate_model_return\"][\"accuracy\"])\n",
    "    return {\"accuracy\": np.mean(metrics)}\n",
    "```\n",
    "Note that we are using the `ClassificationExperiment` defined on the previous example. We are also assuming that `n_bagging` is a combination defined in the `_get_combinations_names` method.\n",
    "\n",
    "```python\n",
    "def _get_combinations_names(self) -> dict:\n",
    "    return [\"n_bagging\"]\n",
    "```\n",
    "\n",
    "The `get_search_space` method is used to define the search space for the hyperparameters that we want to optimize. It should return a dictionary with the keys as the name of the parameter and the values as the `optuna.distributions` distribution that we want to use for the parameter. For example, if we want to optimize the `learning_rate` parameter of the `GradientBoostingClassifier`, we can define the search space as follows:\n",
    "\n",
    "```python\n",
    "def get_search_space(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "    return {\n",
    "        \"learning_rate\": optuna.distributions.FloatDistribution(0.01, 0.5)\n",
    "    }\n",
    "```\n",
    "\n",
    "The `get_default_values` method is used to define some configurations of hyperparameters that we want to evaluate before others, it should return a list of dicts with the keys as the name of the parameter and the values as the value of the parameter. For example, if we want to evaluate a `learning_rate` of 0.1 before others, we can define the default values as follows:\n",
    "\n",
    "```python\n",
    "def get_default_values(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> list:\n",
    "    return [\n",
    "        {\n",
    "            \"learning_rate\": 0.1\n",
    "        }\n",
    "    ]\n",
    "```\n",
    "\n",
    "For some cases, we might want to define a custom `get_trial_fn`. By default this function is defined as follows:\n",
    "\n",
    "```python\n",
    "def get_trial_fn(\n",
    "        self, \n",
    "        study: Study,\n",
    "        search_space: dict, \n",
    "        combination: dict,\n",
    "        unique_params: dict,\n",
    "        extra_params: dict,\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        child_runs_ids: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ) -> dict:\n",
    "        flatten_search_space = flatten_dict(search_space)\n",
    "        trial = study.ask(flatten_search_space)\n",
    "        trial_number = trial.number\n",
    "        trial_key = \"_\".join([str(value) for value in combination.values()])\n",
    "        trial_key = trial_key + f\"-{trial_number}\"  # unique key (trial number)\n",
    "        child_run_id = child_runs_ids[trial_number] if child_runs_ids else None\n",
    "        trial.set_user_attr('child_run_id', child_run_id)\n",
    "        return dict(trial=trial, trial_key=trial_key, child_run_id=child_run_id)\n",
    "```\n",
    "The return of this function is the `trial_dict` passed to the `training_fn` method.\n",
    "\n",
    "Note that by default we pass a `child_rin_id` that can be used to log parameters and metrics to mlflow in a child run attached to the main mlflow run. This is useful to keep track of the different trials and their results in a more organized way, but the user must manually log the parameters and metrics that they want, for example:\n",
    "\n",
    "```python\n",
    "def training_fn(self, trial_dict: dict, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "    n_bagging = unique_params[\"n_bagging\"]\n",
    "    trial = trial_dict[\"trial\"]\n",
    "    learning_rate = trial.params[\"learning_rate\"]\n",
    "    child_run_id = trial_dict[\"child_run_id\"]\n",
    "\n",
    "    if child_run_id is not None:\n",
    "        mlflow.log_params(trial.params, run_id=child_run_id)\n",
    "\n",
    "    metrics = []\n",
    "    for i in range(n_bagging):\n",
    "        base_experiment = ClassificationExperiment(\n",
    "            seed=i,\n",
    "            n_estimators=100,\n",
    "            learning_rate=learning_rate,\n",
    "            model_verbose=0\n",
    "        )\n",
    "        result = base_experiment.run(return_results=True)\n",
    "        metrics.append(result[0][\"evaluate_model_return\"][\"accuracy\"])\n",
    "\n",
    "    mean_metric = np.mean(metrics)\n",
    "    if child_run_id is not None:\n",
    "        mlflow.log_metric(\"accuracy\", mean_metric, run_id=child_run_id)\n",
    "\n",
    "    return {\"accuracy\": mean_metric}\n",
    "```\n",
    "\n",
    "Finally, sometimes it makes sense to define a hpo experiment that inherits from a base experiment (so we can for example share the same initialization logic, and maybe couple some parameters of the HPO experiment to the base experiment). In this case, note that we should inherit first from the `HPOExperiment` class and then from the base experiment class, otherwise we will not being using the methods defined in the `HPOExperiment` class, but the ones defined in the base experiment class, which will not work as expected. For example, if we want to define a `HPOClassificationExperiment` that inherits from the `HPOExperiment` and the `ClassificationExperiment`, we can do it as follows:\n",
    "\n",
    "```python\n",
    "class HPOClassificationExperiment(HPOExperiment, ClassificationExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        n_bagging: int = 5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_bagging = n_bagging\n",
    "\n",
    "    def _get_combinations_names(self) -> list[str]:\n",
    "        combination_names = super()._get_combinations_names()\n",
    "        combination_names.append(\"n_bagging\")\n",
    "        return combination_names\n",
    "\n",
    "    def training_fn(self, trial_dict: dict, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "        n_bagging = unique_params[\"n_bagging\"]\n",
    "        trial = trial_dict[\"trial\"]\n",
    "        learning_rate = trial.params[\"learning_rate\"]\n",
    "        n_estimators = combination[\"n_estimators\"]\n",
    "        child_run_id = trial_dict[\"child_run_id\"]\n",
    "\n",
    "        if child_run_id is not None:\n",
    "            mlflow.log_params(trial.params, run_id=child_run_id)\n",
    "\n",
    "        metrics = []\n",
    "        for i in range(n_bagging):\n",
    "            base_experiment = ClassificationExperiment(\n",
    "                seed=i,\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                model_verbose=0\n",
    "            )\n",
    "            result = base_experiment.run(return_results=True)\n",
    "            metrics.append(result[0][\"evaluate_model_return\"][\"accuracy\"])\n",
    "\n",
    "        mean_metric = np.mean(metrics)\n",
    "        if child_run_id is not None:\n",
    "            mlflow.log_metric(\"accuracy\", mean_metric, run_id=child_run_id)\n",
    "\n",
    "        return {\"accuracy\": mean_metric}\n",
    "\n",
    "```\n",
    "Note that the the methods from the base_experiment `_load_model`, `_get_metrics`, `_fit_model` and `_evaluate_model` are being overriden by the `HPOExperiment` class, however the combinations are being extended, so `seed` and `n_estimators` are still being defined as combinations and so we are iterating through them and performing the HPO process several times with different `n_estimaors` and `seed` values. The values of `seed` are not used anywhere, so this will produce identical results, therefore we should not pass several `seed` values, or get it out of the combination_names. We could also change its definition to a unique_param and iterate through them in the `training_fn` method. Anyway, we should take care when inheriting from both the `HPOExperiment` and the base experiment class, as we might end up with unexpected results if we are not careful with which methods are being overriden or extended, but this is a powerful feature that allows us to create more complex experiments by combining the base experiment logic with the HPO logic.\n",
    "\n",
    "Like `BaseExperiment` some common parameters are already defined in the `HPOExperiment` class, they are:\n",
    "\n",
    "- `n_trials`: the number of trials to run in the HPO process.\n",
    "- `timeout_hpo`: the maximum time in seconds to run the HPO process.\n",
    "- `timeout_trial`: the maximum time in seconds to run each trial.\n",
    "- `max_concurrent_trials`: the maximum number of concurrent trials to run if we are using a distributed setup (dask).\n",
    "- `hpo_seed`: the seed to use for the HPO process, this is used to set the seed for the `optuna` library (sampler, pruner, etc).\n",
    "- `sampler`: the sampler to use for the HPO process, it can be 'tpe', 'random' or 'grid'. If not specified, it will use the 'tpe' sampler.\n",
    "- `pruner`: the pruner to use for the HPO process. It defaults to 'none', but it can be set to 'hyperband' or 'sha', but it is expected that the user will implement the interface between the model and optuna to perform the pruning.\n",
    "- `hpo_metric`: the metric to optimize in the HPO process, it should be one of the metrics returned by the `training_fn` method.\n",
    "- `direction`: the direction to optimize the metric, it can be 'minimize' or 'maximize', it defaults to 'minimize'.\n",
    "\n",
    "Let's now illustrate the full implementation of the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68f1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_experiments import BaseExperiment\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class ClassificationExperiment(BaseExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        seed: int | list[int] = 42,\n",
    "        n_estimators: int | list[int] = 100,\n",
    "        learning_rate: float = 0.1,\n",
    "        model_verbose: int = 1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.seed = seed\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_verbose = model_verbose\n",
    "\n",
    "    def _add_arguments_to_parser(self):\n",
    "        self.parser.add_argument(\n",
    "            \"--seed\",\n",
    "            type=int,\n",
    "            nargs=\"+\",\n",
    "            default=self.seed,\n",
    "            help=\"Random seed for reproducibility.\",\n",
    "        )\n",
    "        self.parser.add_argument(\n",
    "            \"--n_estimators\",\n",
    "            type=int,\n",
    "            nargs=\"+\",\n",
    "            default=self.n_estimators,\n",
    "            help=\"Number of estimators for the model.\",\n",
    "        )\n",
    "        self.parser.add_argument(\n",
    "            \"--learning_rate\",\n",
    "            type=float,\n",
    "            default=self.learning_rate,\n",
    "            help=\"Learning rate for the model.\",\n",
    "        )\n",
    "        self.parser.add_argument(\n",
    "            \"--model_verbose\",\n",
    "            type=int,\n",
    "            default=self.model_verbose,\n",
    "            help=\"Verbosity level of the model training.\",\n",
    "            action=\"store_true\",\n",
    "        )\n",
    "\n",
    "    def _unpack_parser(self):\n",
    "        args = super()._unpack_parser()\n",
    "        self.seed = args.seed\n",
    "        self.n_estimators = args.n_estimators\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.model_verbose = args.model_verbose\n",
    "\n",
    "    def _get_combinations_names(self) -> list[str]:\n",
    "        return [\"seed\", \"n_estimators\"]\n",
    "\n",
    "    def _get_unique_params(self):\n",
    "        unique_params = super()._get_unique_params()\n",
    "        unique_params.update(\n",
    "            {\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "            }\n",
    "        )\n",
    "        return unique_params\n",
    "\n",
    "    def _get_extra_params(self):\n",
    "        extra_params = super()._get_extra_params()\n",
    "        extra_params.update(\n",
    "            {\n",
    "                \"model_verbose\": self.model_verbose,\n",
    "            }\n",
    "        )\n",
    "        return extra_params\n",
    "\n",
    "    def _load_data(\n",
    "        self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs\n",
    "    ):\n",
    "        seed = combination[\"seed\"]\n",
    "        iris = load_iris()\n",
    "        X, y = iris.data, iris.target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "        return dict(\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "        )\n",
    "\n",
    "    def _load_model(\n",
    "        self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs\n",
    "    ):\n",
    "        n_estimators = combination[\"n_estimators\"]\n",
    "        learning_rate = unique_params[\"learning_rate\"]\n",
    "        verbose = extra_params[\"model_verbose\"]\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        return dict(model=model)\n",
    "\n",
    "    def _get_metrics(\n",
    "        self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs\n",
    "    ):\n",
    "        return dict(accuracy=accuracy_score)\n",
    "\n",
    "    def _fit_model(\n",
    "        self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs\n",
    "    ):\n",
    "        model = kwargs[\"load_model_return\"][\"model\"]\n",
    "        X_train = kwargs[\"load_data_return\"][\"X_train\"]\n",
    "        y_train = kwargs[\"load_data_return\"][\"y_train\"]\n",
    "        model.fit(X_train, y_train)\n",
    "        return dict()\n",
    "\n",
    "    def _evaluate_model(\n",
    "        self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs\n",
    "    ):\n",
    "        accuracy_fn = kwargs[\"get_metrics_return\"][\"accuracy\"]\n",
    "        model = kwargs[\"load_model_return\"][\"model\"]\n",
    "        X_test = kwargs[\"load_data_return\"][\"X_test\"]\n",
    "        y_test = kwargs[\"load_data_return\"][\"y_test\"]\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_fn(y_test, y_pred)\n",
    "        return dict(accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd4f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_experiments import HPOExperiment\n",
    "import mlflow\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class HPOClasificationExperiment(HPOExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        n_bagging: int = 1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_bagging = n_bagging\n",
    "\n",
    "    def _get_combinations_names(self) -> list[str]:\n",
    "        return [\"n_bagging\"]\n",
    "\n",
    "    def _get_extra_params(self):\n",
    "        return super()._get_extra_params() \n",
    "\n",
    "    def _load_data(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs):\n",
    "        return dict()\n",
    "\n",
    "    def training_fn(self, trial_dict: dict, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "        n_bagging = combination[\"n_bagging\"]\n",
    "        trial = trial_dict[\"trial\"]\n",
    "        learning_rate = trial.params[\"learning_rate\"]\n",
    "        child_run_id = trial_dict[\"child_run_id\"]\n",
    "\n",
    "        if child_run_id is not None:\n",
    "            mlflow.log_params(trial.params, run_id=child_run_id)\n",
    "\n",
    "        metrics = []\n",
    "        for i in range(n_bagging):\n",
    "            base_experiment = ClassificationExperiment(\n",
    "            seed=i,\n",
    "            n_estimators=100,\n",
    "            learning_rate=learning_rate,\n",
    "            model_verbose=0,\n",
    "            verbose=0\n",
    "            )\n",
    "            result = base_experiment.run(return_results=True)\n",
    "            metrics.append(result[0][\"evaluate_model_return\"][\"accuracy\"])\n",
    "\n",
    "        mean_metric = np.mean(metrics)\n",
    "        if child_run_id is not None:\n",
    "            mlflow.log_metric(\"accuracy\", mean_metric, run_id=child_run_id)\n",
    "\n",
    "        return {\"accuracy\": mean_metric}\n",
    "\n",
    "    def get_search_space(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> dict:\n",
    "        return {\n",
    "            \"learning_rate\": optuna.distributions.FloatDistribution(0.01, 0.5)\n",
    "        }\n",
    "\n",
    "    def get_default_values(self, combination: dict, unique_params: dict, extra_params: dict, mlflow_run_id: str | None = None, **kwargs) -> list:\n",
    "        return [\n",
    "            {\n",
    "                \"learning_rate\": 0.1,\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fdaa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af6a1db4f2f4972b0fd6575918a5bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinations completed:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11a5b0951164563a7b01f60959b0cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ec460110b44e66a8b797dcc9fd790a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trials:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = HPOClasificationExperiment(n_bagging=[5,2], hpo_metric=\"accuracy\", direction=\"maximize\", n_trials=10, verbose=1)\n",
    "result = experiment.run(return_results=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af54764f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work_dir': PosixPath('/home/belucci/code/ml_experiments/work/2f1a82f8c5'),\n",
       " 'save_dir': None,\n",
       " 'load_model_return': {'tuner': <ml_experiments.tuners.OptunaTuner at 0x754ff005acd0>},\n",
       " 'max_memory_used_before_fit': 282.06,\n",
       " 'fit_model_return': {'study': <optuna.study.study.Study at 0x754ff113ef50>,\n",
       "  'elapsed_time': 9.52416181999979},\n",
       " 'max_memory_used_after_fit': 284.62,\n",
       " 'evaluate_model_return': {'best/accuracy': np.float64(0.9733333333333334),\n",
       "  'best/value': 0.9733333333333334},\n",
       " 'total_elapsed_time': 9.525035740000021,\n",
       " 'combination': {'n_bagging': 5},\n",
       " 'unique_params': {'timeout_fit': None,\n",
       "  'timeout_combination': None,\n",
       "  'hpo_framework': 'optuna',\n",
       "  'n_trials': 10,\n",
       "  'timeout_hpo': 0,\n",
       "  'timeout_trial': 0,\n",
       "  'max_concurrent_trials': 1,\n",
       "  'hpo_seed': 0,\n",
       "  'sampler': 'tpe',\n",
       "  'pruner': 'none',\n",
       "  'direction': 'maximize',\n",
       "  'hpo_metric': 'accuracy'},\n",
       " 'extra_params': {},\n",
       " 'mlflow_run_id': None,\n",
       " 'Finished': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohirf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
